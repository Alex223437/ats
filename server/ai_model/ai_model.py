import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, log_loss
from server.data.data_preprocessing import preprocess_data, create_labels
from server.data.market_data import MarketData

MODEL_PATH = "server/ai_model/ai_model.pkl"
LAST_TRAIN_PATH = "server/ai_model/last_train.txt"
TRAIN_INTERVAL_DAYS = 7  # üîÑ –†–∞–∑ –≤ –Ω–µ–¥–µ–ª—é

def should_retrain():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å"""
    if not os.path.exists(LAST_TRAIN_PATH):
        return True  # –§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–Ω–∞—á–∏—Ç, –º–æ–¥–µ–ª—å –µ—â–µ –Ω–µ –æ–±—É—á–∞–ª–∏

    with open(LAST_TRAIN_PATH, "r") as f:
        last_train_date = datetime.strptime(f.read().strip(), "%Y-%m-%d")

    return (datetime.now() - last_train_date).days >= TRAIN_INTERVAL_DAYS

def save_train_date():
    """–°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞—Ç—É –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    with open(LAST_TRAIN_PATH, "w") as f:
        f.write(datetime.now().strftime("%Y-%m-%d"))

def evaluate_model(model, X_test, y_test, feature_names):
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    y_pred = model.predict(X_test)

    if hasattr(model, "predict_proba"):
        y_pred_prob = np.array(model.predict_proba(X_test))

        print("üîç Shape of y_pred_prob:", y_pred_prob.shape)

        if y_pred_prob.ndim == 3:
            y_pred_prob = np.mean(y_pred_prob, axis=0)

        print("üìâ Log Loss:", log_loss(y_test, y_pred_prob))
    else:
        print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba(), log_loss –Ω–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è")

    print("üìä Accuracy:", accuracy_score(y_test, y_pred))
    print("üîé Precision:", precision_score(y_test, y_pred, average="weighted"))
    print("üìâ Recall:", recall_score(y_test, y_pred, average="weighted"))
    print("üìä F1-score:", f1_score(y_test, y_pred, average="weighted"))

    print(classification_report(y_test, y_pred))

    # üî• –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π
    feature_importance = model.feature_importances_

    # üîç –§–∏–∫—Å –æ—à–∏–±–∫–∏: –ø–æ–¥–≥–æ–Ω—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–º–µ–Ω —Ñ–∏—á–µ–π –ø–æ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á–µ–π –≤ –º–æ–¥–µ–ª–∏
    feature_names = feature_names[:len(feature_importance)]  

    plt.figure(figsize=(10, 5))
    plt.barh(feature_names, feature_importance, color='skyblue')
    plt.xlabel("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    plt.ylabel("–§–∏—á–∏")
    plt.title("Feature Importance –≤ RandomForest")
    plt.show()


def train_model(tickers=["AAPL", "TSLA", "NVDA", "GOOGL", "AMZN"]):
    """–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å Random Forest —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    if not should_retrain():
        print("‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞ –Ω–µ–¥–∞–≤–Ω–æ, –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return
    all_data = []
    for ticker in tickers:
        try:
            data = MarketData.download_data(ticker)
            if data is None or data.empty:
                print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {ticker}, –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç")
                continue

            data = create_labels(data)
            data["Ticker"] = ticker  # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–∫–µ—Ä –∫–∞–∫ —Ñ–∏—á—É
            all_data.append(data)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {ticker}: {e}")

    if not all_data:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞")
    
    full_data = pd.concat(all_data, axis=0)
    
    # One-hot encoding –¥–ª—è —Ç–∏–∫–µ—Ä–æ–≤
    encoder = OneHotEncoder(sparse_output=False, drop='first')
    ticker_encoded = encoder.fit_transform(full_data[['Ticker']])
    ticker_df = pd.DataFrame(ticker_encoded, columns=encoder.get_feature_names_out(['Ticker']))

    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–µ—Ä–µ–¥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ–º
    full_data = full_data.drop(columns=['Ticker']).reset_index(drop=True)
    ticker_df = ticker_df.reset_index(drop=True)

    # –û–±—ä–µ–¥–∏–Ω—è–µ–º DataFrame
    full_data = pd.concat([full_data, ticker_df], axis=1)

    print("üìä –ö–æ–ª–æ–Ω–∫–∏ –ø–µ—Ä–µ–¥ preprocess_data():", full_data.columns)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π –î–û –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    feature_names = full_data.drop(columns=['Buy_Signal', 'Sell_Signal']).columns.tolist()

    X, y = preprocess_data(full_data)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–¥–±–æ—Ä–∞
    param_grid = {
        "n_estimators": [50, 100],
        "max_depth": [None, 10],
        "min_samples_split": [2, 5],
        "min_samples_leaf": [1, 2]
    }

    model = RandomForestClassifier(random_state=42)
    
    # –ü–æ–¥–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–π –≤—ã–±–æ—Ä–∫–µ
    grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)
    grid_search.fit(X_train[:1000], y_train[:1000])  # ‚ö° –£—Å–∫–æ—Ä—è–µ–º –æ–±—É—á–µ–Ω–∏–µ

    best_model = grid_search.best_estimator_

    # üî• –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ + –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ —Ñ–∏—á–µ–π
    evaluate_model(best_model, X_test, y_test, feature_names)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    with open(MODEL_PATH, "wb") as f:
        pickle.dump(best_model, f)

    save_train_date()

    print("üéØ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

if __name__ == "__main__":
    train_model()