import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, log_loss
import joblib

from data.data_preprocessing import preprocess_data, create_labels, prepare_features_for_ml
from data.market_data import MarketData

MODEL_PATH = "services/ml_models/models/rf_model.pkl"
SCALER_PATH = "services/ml_models/models/scaler_rf.pkl"
LAST_TRAIN_PATH = "services/ml_models/models/last_train.txt"
TRAIN_INTERVAL_DAYS = 7

def should_retrain():
    if not os.path.exists(LAST_TRAIN_PATH):
        return True
    with open(LAST_TRAIN_PATH, "r") as f:
        last_train_date = datetime.strptime(f.read().strip(), "%Y-%m-%d")
    return (datetime.now() - last_train_date).days >= TRAIN_INTERVAL_DAYS

def save_train_date():
    with open(LAST_TRAIN_PATH, "w") as f:
        f.write(datetime.now().strftime("%Y-%m-%d"))

def evaluate_model(model, X_test, y_test, feature_names):
    y_pred = model.predict(X_test)
    if hasattr(model, "predict_proba"):
        y_pred_prob = np.array(model.predict_proba(X_test))
        if y_pred_prob.ndim == 3:
            y_pred_prob = np.mean(y_pred_prob, axis=0)
        print("üìâ Log Loss:", log_loss(y_test, y_pred_prob))
    print("üìä Accuracy:", accuracy_score(y_test, y_pred))
    print("üîé Precision:", precision_score(y_test, y_pred, average="weighted"))
    print("üìâ Recall:", recall_score(y_test, y_pred, average="weighted"))
    print("üìä F1-score:", f1_score(y_test, y_pred, average="weighted"))
    print(classification_report(y_test, y_pred))
    feature_importance = model.feature_importances_
    feature_names = feature_names[:len(feature_importance)]
    plt.figure(figsize=(10, 5))
    plt.barh(feature_names, feature_importance, color='skyblue')
    plt.xlabel("–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    plt.ylabel("–§–∏—á–∏")
    plt.title("Feature Importance –≤ RandomForest")
    plt.show()

def train_model(tickers=["AAPL", "TSLA", "NVDA", "GOOGL", "AMZN"]):
    if not should_retrain():
        print("‚úÖ –ú–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞ –Ω–µ–¥–∞–≤–Ω–æ, –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return

    all_data = []
    for ticker in tickers:
        try:
            data = MarketData.download_data(ticker)
            if data is None or data.empty:
                print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {ticker}, –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç")
                continue
            data = create_labels(data)  # –¥–æ–±–∞–≤–∏—Ç Target: -1, 0, 1
            data["Ticker"] = ticker
            all_data.append(data)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {ticker}: {e}")

    if not all_data:
        raise ValueError("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –Ω–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞")

    full_data = pd.concat(all_data, axis=0)

    # One-hot encoding –¥–ª—è —Ç–∏–∫–µ—Ä–æ–≤
    encoder = OneHotEncoder(sparse_output=False, drop='first')
    ticker_encoded = encoder.fit_transform(full_data[['Ticker']])
    ticker_df = pd.DataFrame(ticker_encoded, columns=encoder.get_feature_names_out(['Ticker']))

    full_data = full_data.drop(columns=['Ticker']).reset_index(drop=True)
    ticker_df = ticker_df.reset_index(drop=True)
    full_data = pd.concat([full_data, ticker_df], axis=1)

    # X ‚Äî –±–µ–∑ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è, y = Target
    X, y = preprocess_data(full_data)
    if y is None or y.empty:
        raise ValueError("‚ùå –ù–µ—Ç —Ç–∞—Ä–≥–µ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (Target)")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, SCALER_PATH)

    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    param_grid = {
        "n_estimators": [50, 100],
        "max_depth": [None, 10],
        "min_samples_split": [2, 5],
        "min_samples_leaf": [1, 2]
    }

    model = RandomForestClassifier(random_state=42)
    grid_search = GridSearchCV(model, param_grid, cv=3, n_jobs=-1, verbose=2)
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_

    evaluate_model(best_model, X_test, y_test, X.columns.tolist())

    with open(MODEL_PATH, "wb") as f:
        pickle.dump(best_model, f)

    save_train_date()
    print("üéØ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")

if __name__ == "__main__":
    train_model()

